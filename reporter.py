# reporter.py

from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any
from collections import defaultdict
import json
from pathlib import Path

# --- å®šç¾©æª”æ¡ˆè·¯å¾‘ ---
CWD = Path(__file__).parent
LOG_FILE = CWD / "log.json"
TASKS_FILE = CWD / "tasks.json"
SUBJECTS_FILE = CWD / "subjects.json"

def _load_json_data(filepath: Path) -> List[Dict[str, Any]]:
    """ä¸€å€‹è¼”åŠ©å‡½å¼ï¼Œç”¨æ–¼è¼‰å…¥ JSON æª”æ¡ˆä¸­çš„åˆ—è¡¨è³‡æ–™ã€‚"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, dict):
                # è™•ç†å¯èƒ½çš„ JSON çµæ§‹: {"logs": [...]} æˆ– {"subjects": [...]}
                key = next(iter(data))
                return data.get(key, [])
            return data
    except (FileNotFoundError, IndexError, json.JSONDecodeError, StopIteration):
        return []

def generate_weekly_report_ascii() -> str:
    """
    è®€å–æ—¥èªŒèˆ‡ä»»å‹™æª”æ¡ˆï¼Œç”¢ç”Ÿä¸€ä»½ ASCII æ ¼å¼çš„æœ¬é€±å­¸ç¿’å ±å‘Šã€‚
    å ±å‘ŠåŒ…å«å„ç§‘å­¸ç¿’æ™‚é–“é•·æ¢åœ–ï¼Œä¸¦æ¨™ç¤ºå‡ºå¼±é»ç§‘ç›®ã€‚

    Returns:
        ä¸€å€‹æ ¼å¼åŒ–å¾Œå¯ç”¨æ–¼çµ‚ç«¯æ©Ÿè¼¸å‡ºçš„å­—ä¸²ã€‚
    """
    logs = _load_json_data(LOG_FILE)
    tasks = _load_json_data(TASKS_FILE)
    subjects = _load_json_data(SUBJECTS_FILE)

    if not logs or not tasks or not subjects:
        return "å°šç„¡è¶³å¤ çš„è³‡æ–™å¯ç”¢ç”Ÿå ±å‘Šã€‚"

    # å»ºç«‹æŸ¥æ‰¾å­—å…¸ä»¥æé«˜æ•ˆç‡
    task_to_subject_map = {task['task_id']: task['subject_id'] for task in tasks}
    subject_id_to_name_map = {s['id']: s.get('name', 'æœªçŸ¥ç§‘ç›®') for s in subjects}

    # è¨ˆç®—æœ¬é€±æ•¸æ“š
    today = datetime.now(timezone.utc).date()
    start_of_week = today - timedelta(days=today.weekday())
    
    time_distribution = defaultdict(float)
    bad_counts = defaultdict(int)

    for log in logs:
        try:
            log_timestamp = datetime.fromisoformat(log['timestamp'].replace('Z', '+00:00'))
            log_date = log_timestamp.date()

            if log_date >= start_of_week:
                task_id = log.get('task_id')
                subject_id = task_to_subject_map.get(task_id)
                if subject_id:
                    time_distribution[subject_id] += float(log.get('duration_minutes', 0))
                    if log.get('performance') == 'bad':
                        bad_counts[subject_id] += 1
        except (ValueError, TypeError, KeyError):
            continue

    if not time_distribution:
        return "æœ¬é€±å°šç„¡å­¸ç¿’ç´€éŒ„ã€‚"

    # æ‰¾å‡ºå¼±é»ç§‘ç›®
    weakest_subject_id = None
    if bad_counts:
        weakest_subject_id = max(bad_counts, key=bad_counts.get)

    # æº–å‚™ç¹ªè£½é•·æ¢åœ–
    report_lines = []
    report_lines.append(f"--- ğŸ“Š æœ¬é€±å­¸ç¿’æ™‚é–“åˆ†ä½ˆå ±å‘Š (è‡ª {start_of_week.strftime('%Y-%m-%d')} èµ·) ---")
    
    max_minutes = max(time_distribution.values()) if time_distribution else 1
    max_bar_width = 40  # é•·æ¢åœ–æœ€å¤§å¯¬åº¦

    # æ ¹æ“šå­¸ç¿’æ™‚é–“æ’åº
    sorted_subjects = sorted(time_distribution.items(), key=lambda item: item[1], reverse=True)

    for subject_id, total_minutes in sorted_subjects:
        subject_name = subject_id_to_name_map.get(subject_id, subject_id)
        hours = total_minutes / 60.0
        
        bar_width = int((total_minutes / max_minutes) * max_bar_width)
        bar = "â–ˆ" * bar_width

        marker = "ğŸ”¥" if subject_id == weakest_subject_id else ""
        
        line = f"{subject_name:<6s} | {hours:4.1f} hrs | {bar} {marker}"
        report_lines.append(line)
        
    if weakest_subject_id:
        weakest_subject_name = subject_id_to_name_map.get(weakest_subject_id, weakest_subject_id)
        report_lines.append("\n" + "="*50)
        report_lines.append(f"ğŸ”¥ å¼±é»æé†’ï¼šæœ¬é€±ã€Œ{weakest_subject_name}ã€çš„ 'bad' è©•ç­‰æ¬¡æ•¸æœ€å¤šï¼Œè«‹å¤šåŠ æ³¨æ„ï¼")

    return "\n".join(report_lines)